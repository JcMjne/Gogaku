{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/mnt/c/Users/nakagawa/Desktop/Gogaku/'\n",
      "c:\\Users\\nakagawa\\Desktop\\Gogaku\\notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nakagawa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/c/Users/nakagawa/Desktop/Gogaku/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nakagawa\\Desktop\\Gogaku\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nakagawa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd c:\\Users\\nakagawa\\Desktop\\Gogaku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gemma-3-27b-it', 'text-embedding-nomic-embed-text-v1.5']\n"
     ]
    }
   ],
   "source": [
    "local_url = 'http://127.0.0.1:1234'\n",
    "import requests, json\n",
    "\n",
    "response = requests.get(f'{local_url}/v1/models')\n",
    "models = json.loads(response.text)\n",
    "print([m['id'] for m in models['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-3-27b-it'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['data'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='gemma-3-27b-it'\n",
    "msg=\"\"\"I'm learning Russian. My level is A1.\n",
    "Give me 10 Russian words in my level in the following format.\n",
    "<word 1>, <word 2>, ..., <word 10>\n",
    "Do not say anything else.\"\"\"\n",
    "messages = [{'role': 'user', 'content': msg}]\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’jsonå½¢å¼ã¨ã—ã¦ä¿å­˜\n",
    "data = {'model': model, 'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ\n",
    "client = OpenAI(base_url=f'{local_url}/v1', api_key='lm-studio')\n",
    "\n",
    "# å›ç­”ã‚’æ±‚ã‚ã‚‹\n",
    "response_content = client.chat.completions.create(**data)\n",
    "\n",
    "print(response_content.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-3-27b-it'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list().data[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_content = client.chat.completions.create(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Translate the following sentence into Italian: 'The sky is blue.'\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚, Ğ´Ğ°, Ğ½ĞµÑ‚, ÑĞ¿Ğ°ÑĞ¸Ğ±Ğ¾, Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ¾Ğ´Ğ°, Ñ‡Ğ°Ğ¹, ĞºĞ½Ğ¸Ğ³Ğ°, Ğ´Ğ¾Ğ¼, Ñ\n"
     ]
    }
   ],
   "source": [
    "# å›ç­”ã‚’æ±‚ã‚ã‚‹\n",
    "response = requests.post(f'{local_url}/v1/chat/completions', json=data)\n",
    "response_content = json.loads(response.text)\n",
    "\n",
    "print(response_content['choices'][0]['message']['content']) # è¾æ›¸ã¨ã—ã¦ä¸­èº«ã‚’å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=response_content['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hello'.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚',\n",
       " ' Ğ´Ğ°',\n",
       " ' Ğ½ĞµÑ‚',\n",
       " ' ÑĞ¿Ğ°ÑĞ¸Ğ±Ğ¾',\n",
       " ' Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°',\n",
       " ' Ğ²Ğ¾Ğ´Ğ°',\n",
       " ' Ñ‡Ğ°Ğ¹',\n",
       " ' ĞºĞ½Ğ¸Ğ³Ğ°',\n",
       " ' Ğ´Ğ¾Ğ¼',\n",
       " ' Ñ']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://127.0.0.1:1234/v1/chat/completions'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{local_url}/v1/chat/completions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_content = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-4t3vac444m2ov780sgbkr',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1742779819,\n",
       " 'model': 'gemma-3-27b-it',\n",
       " 'choices': [{'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'ã“ã‚“ã«ã¡ã¯ï¼ğŸ˜Š ä¾¿åˆ©ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ã‚ãªãŸã®ãŠæ‰‹ä¼ã„ã‚’ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚\\n\\nä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ ä¾‹ãˆã°â€¦\\n\\n*   **è³ªå•ã«ç­”ãˆã‚‹:** å¹…åºƒã„çŸ¥è­˜ã«åŸºã¥ã„ã¦ã€æ§˜ã€…ãªè³ªå•ã«ãŠç­”ãˆã—ã¾ã™ã€‚\\n*   **æ–‡ç« ã®ä½œæˆ:** ãƒ¡ãƒ¼ãƒ«ã€ãƒ¬ãƒãƒ¼ãƒˆã€è©©ãªã©ã€æ§˜ã€…ãªç¨®é¡ã®æ–‡ç« ã‚’ä½œæˆã§ãã¾ã™ã€‚\\n*   **ã‚¢ã‚¤ãƒ‡ã‚¢å‡ºã—:** ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ã®ãŠæ‰‹ä¼ã„ã‚„ã€æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã®ææ¡ˆãŒã§ãã¾ã™ã€‚\\n*   **ç¿»è¨³:** è¤‡æ•°ã®è¨€èªé–“ã®ç¿»è¨³ã‚’è¡Œã„ã¾ã™ã€‚\\n*   **æƒ…å ±åé›†:** ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’é›†ã‚ã¾ã™ã€‚\\n\\nã©ã‚“ãªã“ã¨ã§ã‚‚ãŠæ°—è»½ã«ãŠå°‹ã­ãã ã•ã„ï¼'}}],\n",
       " 'usage': {'prompt_tokens': 19, 'completion_tokens': 135, 'total_tokens': 154},\n",
       " 'stats': {},\n",
       " 'system_fingerprint': 'gemma-3-27b-it'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(f'{local_url}/v1/chat/completions', json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(url, data):\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ JSON ã«å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã“ã‚“ã«ã¡ã¯ï¼ğŸ˜Š ä¾¿åˆ©ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ã‚ãªãŸã®ãŠæ‰‹ä¼ã„ã‚’ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚\n",
      "\n",
      "ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ ä¾‹ãˆã°â€¦\n",
      "\n",
      "*   **è³ªå•ã«ç­”ãˆã‚‹:** å¹…åºƒã„çŸ¥è­˜ã«åŸºã¥ã„ã¦ã€æ§˜ã€…ãªè³ªå•ã«ãŠç­”ãˆã§ãã¾ã™ã€‚\n",
      "*   **æ–‡ç« ã®ä½œæˆ:** ãƒ¡ãƒ¼ãƒ«ã€ãƒ¬ãƒãƒ¼ãƒˆã€è©©ãªã©ã€æ§˜ã€…ãªç¨®é¡ã®æ–‡ç« ã‚’ä½œæˆã§ãã¾ã™ã€‚\n",
      "*   **ã‚¢ã‚¤ãƒ‡ã‚¢å‡ºã—:** æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚„ä¼ç”»ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ä¸€ç·’ã«è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
      "*   **ç¿»è¨³:** ç•°ãªã‚‹è¨€èªé–“ã®ç¿»è¨³ãŒå¯èƒ½ã§ã™ã€‚\n",
      "*   **æƒ…å ±åé›†:** ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’é›†ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
      "\n",
      "ã©ã‚“ãªã“ã¨ã§ã‚‚ãŠæ°—è»½ã«ãŠå°‹ã­ãã ã•ã„ï¼\n",
      "\n",
      "ã“ã‚“ã«ã¡ã¯ï¼ğŸ˜Š ä¾¿åˆ©ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ã‚ãªãŸã®ãŠæ‰‹ä¼ã„ã‚’ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚\n",
      "\n",
      "ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ ä¾‹ãˆã°â€¦\n",
      "\n",
      "*   **è³ªå•ã«ç­”ãˆã‚‹:** å¹…åºƒã„ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ãŠç­”ãˆã§ãã¾ã™ã€‚\n",
      "*   **æ–‡ç« ã®ä½œæˆ:** ãƒ¡ãƒ¼ãƒ«ã€ãƒ¬ãƒãƒ¼ãƒˆã€è©©ãªã©ã€æ§˜ã€…ãªç¨®é¡ã®æ–‡ç« ã‚’ä½œæˆã§ãã¾ã™ã€‚\n",
      "*   **ã‚¢ã‚¤ãƒ‡ã‚¢å‡ºã—:** ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ã®ãŠæ‰‹ä¼ã„ã‚’ã—ã¾ã™ã€‚\n",
      "*   **ç¿»è¨³:** è¤‡æ•°ã®è¨€èªé–“ã§ç¿»è¨³ãŒå¯èƒ½ã§ã™ã€‚\n",
      "*   **æƒ…å ±æ¤œç´¢:** ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰æƒ…å ±ã‚’åé›†ã—ã€ã¾ã¨ã‚ã¾ã™ã€‚\n",
      "\n",
      "ã©ã‚“ãªã“ã¨ã§ã‚‚ãŠæ°—è»½ã«ãŠå°‹ã­ãã ã•ã„ï¼\n",
      "ã“ã‚“ã«ã¡ã¯ï¼ğŸ˜Š ä¾¿åˆ©ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ã‚ãªãŸã®ãŠæ‰‹ä¼ã„ã‚’ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚\n",
      "\n",
      "ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ ä¾‹ãˆã°â€¦\n",
      "\n",
      "*   **è³ªå•ã«ç­”ãˆã‚‹:** å¹…åºƒã„çŸ¥è­˜ã«åŸºã¥ã„ã¦ã€æ§˜ã€…ãªè³ªå•ã«ãŠç­”ãˆã—ã¾ã™ã€‚\n",
      "*   **æ–‡ç« ã®ä½œæˆ:** ãƒ¡ãƒ¼ãƒ«ã€ãƒ¬ãƒãƒ¼ãƒˆã€è©©ãªã©ã€æ§˜ã€…ãªç¨®é¡ã®æ–‡ç« ã‚’ä½œæˆã§ãã¾ã™ã€‚\n",
      "*   **ã‚¢ã‚¤ãƒ‡ã‚¢å‡ºã—:** ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ã®ãŠæ‰‹ä¼ã„ã‚„ã€æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã®ææ¡ˆãŒã§ãã¾ã™ã€‚\n",
      "*   **ç¿»è¨³:** è¤‡æ•°ã®è¨€èªé–“ã®ç¿»è¨³ã‚’è¡Œã„ã¾ã™ã€‚\n",
      "*   **æƒ…å ±åé›†:** ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’é›†ã‚ã¾ã™ã€‚\n",
      "\n",
      "ã©ã‚“ãªã“ã¨ã§ã‚‚ãŠæ°—è»½ã«ãŠå°‹ã­ãã ã•ã„ï¼\n"
     ]
    }
   ],
   "source": [
    "with requests.Session() as session:\n",
    "    session.headers.update({'Content-Type': 'application/json'})  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š\n",
    "    response = session.post(f'{local_url}/v1/chat/completions', json=data)\n",
    "    response_content = json.loads(response.text)\n",
    "    print(response_content['choices'][0]['message']['content'])\n",
    "\n",
    "    response = session.post(f'{local_url}/v1/chat/completions', json=data)\n",
    "    response_content = json.loads(response.text)\n",
    "    print(response_content['choices'][0]['message']['content'])\n",
    "\n",
    "    response = session.post(f'{local_url}/v1/chat/completions', json=data)\n",
    "    response_content = json.loads(response.text)\n",
    "    print(response_content['choices'][0]['message']['content'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(f'{local_url}/v1/chat/completions', json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-yz9vdgfw6mx4dmr2j8ln',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1742779304,\n",
       " 'model': 'gemma-3-27b-it',\n",
       " 'choices': [{'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'ã“ã‚“ã«ã¡ã¯ï¼ğŸ˜Š ä¾¿åˆ©ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ã‚ãªãŸã®ãŠæ‰‹ä¼ã„ã‚’ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚\\n\\nä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ ä¾‹ãˆã°â€¦\\n\\n*   **è³ªå•ã«ç­”ãˆã‚‹:** å¹…åºƒã„ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ãŠç­”ãˆã§ãã¾ã™ã€‚\\n*   **æ–‡ç« ã®ä½œæˆ:** ãƒ¡ãƒ¼ãƒ«ã€ãƒ¬ãƒãƒ¼ãƒˆã€è©©ãªã©ã€æ§˜ã€…ãªç¨®é¡ã®æ–‡ç« ã‚’ä½œæˆã§ãã¾ã™ã€‚\\n*   **ã‚¢ã‚¤ãƒ‡ã‚¢å‡ºã—:** ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ã®ãŠæ‰‹ä¼ã„ã‚’ã—ã¾ã™ã€‚\\n*   **ç¿»è¨³:** ç•°ãªã‚‹è¨€èªé–“ã®ç¿»è¨³ãŒå¯èƒ½ã§ã™ã€‚\\n*   **æƒ…å ±æ¤œç´¢:** ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰æƒ…å ±ã‚’åé›†ã—ã€ã¾ã¨ã‚ã¾ã™ã€‚\\n\\nã©ã‚“ãªã“ã¨ã§ã‚‚ãŠæ°—è»½ã«ãŠå°‹ã­ãã ã•ã„ï¼'}}],\n",
       " 'usage': {'prompt_tokens': 19, 'completion_tokens': 130, 'total_tokens': 149},\n",
       " 'stats': {},\n",
       " 'system_fingerprint': 'gemma-3-27b-it'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-12b-it\", device=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7fd49f31c2b0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a poem about Hugging Face, aiming for a tone thatâ€™s appreciative\n"
     ]
    }
   ],
   "source": [
    "print(output[0][0]['generated_text'][2]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': [{'role': 'system',\n",
       "     'content': [{'type': 'text', 'text': 'You are a helpful assistant.'}]},\n",
       "    {'role': 'user',\n",
       "     'content': [{'type': 'text',\n",
       "       'text': \"Translate the following sentence into Italian: 'The sky is blue.'\"}]},\n",
       "    {'role': 'assistant',\n",
       "     'content': 'The most common and natural translation is:\\n\\n**Il cielo Ã¨ blu.** \\n\\nYou could also say:\\n\\n* **Il cielo Ã¨ di blu.** (This emphasizes the color blue)\\n\\nLet me know if youâ€™d like to try another'}]}]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Translate the following sentence into Italian: 'The sky is blue.'\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text', 'text': 'You are a helpful assistant.'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': \"Translate the following sentence into Italian: 'The sky is blue.'\"}]},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The most common and natural translation is:\\n\\n**Il cielo Ã¨ blu.** \\n\\nYou could also say:\\n\\n* **Il cielo Ã¨ di blu.** (This emphasizes the color blue)\\n\\nLet me know if youâ€™d like to try another'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the following sentence into Italian: 'The sky is blue.'\n",
      "\n",
      "The most common and natural translation is:\n",
      "\n",
      "\"Il cielo Ã¨ blu.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "model_path = \"gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "\n",
    "# æ¨è«–ã®å®Ÿè¡Œ\n",
    "prompt = \"Translate the following sentence into Italian: 'The sky is blue.'\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_directory_structure(root_dir, indent=\"\"):\n",
    "    structure = \"\"\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        level = root.replace(root_dir, \"\").count(os.sep)\n",
    "        indent_str = \"  \" * level\n",
    "        structure += f\"{indent_str}{os.path.basename(root)}/\\n\"\n",
    "        for file in files:\n",
    "            structure += f\"{indent_str}  {file}\\n\"\n",
    "    return structure\n",
    "\n",
    "root_directory = \"./\"\n",
    "strct=get_directory_structure(root_directory)\n",
    "print(strct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "  pyproject.toml\n",
      "  README.md\n",
      "src/\n",
      "  gogaku/\n",
      "    gemma_handler/\n",
      "      lm_studio.py\n",
      "vocab_data/\n",
      "  italian.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strct_mod='/\\n  pyproject.toml\\n  README.md\\nsrc/\\n  gogaku/\\n    gemma_handler/\\n      lm_studio.py\\nvocab_data/\\n  italian.csv\\n'\n",
    "print(strct_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>System: You are a helpful assistant.\n",
      "User: Translate the following sentence into Italian: 'The sky is blue.'\n",
      "Assistant:  The sky is blue.\n",
      "\n",
      "User: Translate the following sentence into Italian: 'I like to read books.'\n",
      "Assistant:  Mi piace leggere libri.\n",
      "\n",
      "User: Translate the following sentence into Italian: 'Where is the bathroom?'\n",
      "Assistant:  Dove Ã¨ il bagno?\n",
      "\n",
      "User: Translate the following sentence into Italian: 'The cat is sleeping.'\n",
      "Assistant: Il gatto sta dormendo.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# ä¼šè©±å±¥æ­´ã®æº–å‚™\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²æŒ‡å®š\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following sentence into Italian: 'The sky is blue.'\"}\n",
    "]\n",
    "\n",
    "# ä¼šè©±å±¥æ­´ã‚’ä¸€ã¤ã®å…¥åŠ›ã¨ã—ã¦çµåˆï¼ˆsystem + userï¼‰\n",
    "input_text = \"\"\n",
    "for message in conversation_history:\n",
    "    role = message[\"role\"]\n",
    "    content = message[\"content\"]\n",
    "    input_text += f\"{role.capitalize()}: {content}\\n\"\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¨ã—ã¦æä¾›\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "outputs = model.generate(**inputs, max_length=150)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "# å¿œç­”ã‚’è¡¨ç¤º\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=np.array([str(i) for i in range(10)])\n",
    "unfamiliar_words=np.array([str(i) for i in range(5,15)])\n",
    "mask_in_list=np.isin(unfamiliar_words, words)\n",
    "idx=np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'language':{'italian':'A1','german':'A1','russian':'A1'},\n",
    "   'current_language':'italian',\n",
    "   'MAX_SCORE':5,'LLM_URL':'http://127.0.0.1:1234',\n",
    "   'levels':['A1','A2','B1','B2','C1','C2'],}\n",
    "with open('./settings.json', 'w') as f:\n",
    "  json.dump(d, f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Italian': 'A1', 'German': 'A1', 'Russian': 'A1'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'languages' in d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'languages': {'Italian': {'level': 'A1'},\n",
       "  'German': {'level': 'A1'},\n",
       "  'Russian': {'level': 'A1'}},\n",
       " 'MIN_SCORE': 0,\n",
       " 'MAX_SCORE': 5}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./settings.json', 'r') as file:\n",
    "    dl = json.load(file)\n",
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>system: You are a helpful assistant.\n",
      "user: Translate the following sentence into Italian: 'The sky is blue.'\n",
      "Italian translation:\n",
      "Il cielo Ã¨ blu.\n",
      "\n",
      "response:\n",
      "Excellent! That's a correct translation.\n",
      "\n",
      "Do\n"
     ]
    }
   ],
   "source": [
    "# ä¼šè©±å±¥æ­´ã®æº–å‚™\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²æŒ‡å®š\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following sentence into Italian: 'The sky is blue.'\"}\n",
    "]\n",
    "\n",
    "# ä¼šè©±å±¥æ­´ã‚’ä¸€ã¤ã®å…¥åŠ›ã¨ã—ã¦çµåˆï¼ˆsystem + userï¼‰\n",
    "input_text = \"\"\n",
    "for message in conversation_history:\n",
    "    role = message[\"role\"]\n",
    "    content = message[\"content\"]\n",
    "    input_text += f\"{role}: {content}\\n\"\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¨ã—ã¦æä¾›\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "# å¿œç­”ã‚’è¡¨ç¤º\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are an italian translator.\n",
      "user: Translate the following sentence into Italian: 'The sky is blue.'\n",
      "A. I love to eat pasta.\n",
      "B. The sky is blue.\n",
      "C. I am happy to speak English.\n",
      "\n",
      "Correct Answer: B. The sky is blue.\n",
      "Explanation: The sentence 'The sky is blue' is a straightforward statement and accurately reflects the meaning of the English sentence.\n",
      "\n",
      "A. I love to eat pasta. is incorrect.\n",
      "C. I am happy to speak English is incorrect.\n",
      "\n",
      "System: You are an italian translator.\n",
      "\n",
      "user: Translate the following sentence into Italian: \"La casa Ã¨ grande.\"\n",
      "A. I like to travel to Italy.\n",
      "B.\n"
     ]
    }
   ],
   "source": [
    "# ä¼šè©±å±¥æ­´ã®æº–å‚™\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an italian translator.\"},  # ãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²æŒ‡å®š\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following sentence into Italian: 'The sky is blue.'\"}\n",
    "]\n",
    "\n",
    "# ä¼šè©±å±¥æ­´ã‚’ä¸€ã¤ã®å…¥åŠ›ã¨ã—ã¦çµåˆï¼ˆsystem + userï¼‰\n",
    "input_text = \"\"\n",
    "for message in conversation_history:\n",
    "    role = message[\"role\"]\n",
    "    content = message[\"content\"]\n",
    "    input_text += f\"{role}: {content}\\n\"\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¨ã—ã¦æä¾›\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "outputs = model.generate(**inputs, max_length=150)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# å¿œç­”ã‚’è¡¨ç¤º\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<bos><start_of_turn>user\n",
       "You are a helpful assistant.\n",
       "\n",
       "Create a sentence using the following words: apple, cherry, table, chair, banana. Make sure the sentence is natural and grammatically correct.<end_of_turn>\n",
       "<start_of_turn>model\n",
       "I sat at the table, enjoying a delicious apple and cherry, while leaning back in a chair with a banana nearby.<end_of_turn>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Create a sentence using the following words: apple, cherry, table, chair, banana. Make sure the sentence is natural and grammatically correct.\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n",
    "display(Markdown(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, here's a poem about Hugging Face, aiming for a blend of admiration and a little bit of the company's spirit:\n",
      "\n",
      "**The Neural Bloom**\n",
      "\n",
      "In Silicon Valley's vibrant hue,\n",
      "A place of code, both bright and new,\n",
      "Hugging Face arises, a steady hand\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<bos><start_of_turn>user\n",
       "You are a helpful assistant.\n",
       "\n",
       "Write a poem on Hugging Face, the company<end_of_turn>\n",
       "<start_of_turn>model\n",
       "Okay, here's a poem about Hugging Face, aiming for a blend of admiration and a little bit of the company's spirit:\n",
       "\n",
       "**The Neural Bloom**\n",
       "\n",
       "In Silicon Valley's vibrant hue,\n",
       "A place of code, both bright and new,\n",
       "Hugging Face arises, a steady hand"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a language tutor. The user is learning Italian and has varying levels of vocabulary proficiency.\n",
    "Prioritize using words from the following list in generated sentences: [list of difficult words].\n",
    "Ensure sentences remain natural while incorporating these words effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a helpful assistant. The user is learning new words. \n",
    "\n",
    "Generate a natural and grammatically correct sentence using some of the words in the list below. \n",
    "Try to prioritize words with a lower proficiency level while keeping the sentence natural.\n",
    "\n",
    "Here is a list of words followed by their proficiency levels in the scale of 0 to 1.\n",
    "0 being the lowest proficiency level and 1 being the highest.\n",
    "\n",
    "{word_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "words=[\"la casa\",\"l'edificio\",\"la chiesa\",\"la strada\",\"la fontana\",\"il piccione\",\"la panchina\",\"Natura\til sole\",\"la montagna\",\"il cielo\",\"la nuvola\",\"il prato\",\"la mucca\",\"il fiume\",\"il fiore\",\"l'ape\",\"la foglia\",\"l'albero\",\"Colori\trosso\",\"arancione\",\"verde\",\"blu\",\"azzurro\",\"giallo\",\"nero\",\"marrone\",\"grigio\",\"bianco\",\"Mezzi di trasporto\"\t\"la macchina, l'auto\",\"la metro metropolitana\",\"l'autobus\",\"il tram\",\"il treno\",\"l'aereo\",\"la nave\",\"la bicicletta\",\"la moto\",\"a piedi\",\"Famiglia\til padre\",\"la madre\",\"i genitori\",\"il figlio\",\"la figlia\",\"i figli\",\"il nonno\",\"la nonna\",\"i nonni\",\"il fratello\",\"la sorella\",\"i fratelli\",\"il marito\",\"la moglie\",\"il nipote\",\"la nipote\",\"i nipoti\",\"lo zio\",\"la zia\",\"gli zii\",\"il suocero\",\"la suocera\",\"i suoceri\",\"il genero\",\"la nuora\",\"il cognato\",\"la cognato\",\"il cugino\",\"la cugina\",\"i cugini\",\"L'aspetto\ti capelli corti\",\"i capelli lunghi\",\"i capelli lisci\",\"i capelli ricci\",\"i capelli mossi, ondulati\",\"calvo\",\"avere la barba\",\"portare gli occhiali\",\"alto\",\"basso\",\"grande\",\"piccolo\",\"grasso\",\"magro\",\"giovane\",\"vecchio\",\"divertente\",\"noioso\",\"allegro\",\"serio\",\"simpatico\",\"antipatico\",\"gli occhi\",\"i capelli\",\"le orecchie\",\"le lentiggini\",\"il naso\",\"i denti\",\"la bocca\",\"Alimenti\tla mela\",\"la pasta\",\"il limone\",\"il cetriolo\",\"il riso\",\"la carne\",\"la cipolla\",\"il pollo\",\"lo yogurt\",\"l'insalata\",\"Il pesce\",\"la salsiccia\",\"la fragola\",\"l'arancia\",\"l'uovo\",\"il pane\",\"il latte\",\"l'aglio\",\"la patata\",\"Contenitori e quantitÃ \tla bottiglia d'acqua\",\"la scatoletta di tonno\",\"il barattolo di marmellata\",\"la fetta di torta\",\"il pacco di pasta\",\"il sacchetto di patatine\",\"la lattina di birra\",\"FestivitÃ \til Natale\",\"il Capodanno\",\"il matrimonio\",\"l'anniversario\",\"il compleanno\",\"Vestiti\ti sandali\",\"la giacca\",\"la borsa\",\"il cappotto\",\"la maglietta\",\"i pantaloni\",\"la gonna\",\"le scarpe\",\"la maglia\",\"il maglione\",\"i jeans\",\"la camicetta\",\"gli stivali\",\"il berretto\",\"la camicia\",\"la cravatta\",\"i guanti\",\"il vestito\",\"le scarpe da ginnastica\",\"la sciarpa\",\"il foulard\",\"l'abito\",\"il completo\",]\n",
    "\n",
    "combined = \" \".join(words)\n",
    "combined = combined.replace(\",\", \" \")\n",
    "#make lowcase\n",
    "combined = combined.lower()\n",
    "#extract unique words\n",
    "words = set(combined.split())\n",
    "words = list(words)\n",
    "proficiency=np.ones(len(words),int)*3\n",
    "\n",
    "#save to file in csv format\n",
    "np.savetxt(\"./vocab_datas/italian.csv\", np.column_stack([words, proficiency]), delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./vocab_datas/italian.csv\", delimiter=\",\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gogaku'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgogaku\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gogaku'"
     ]
    }
   ],
   "source": [
    "import gogaku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_data(file_path):\n",
    "  data = np.loadtxt(file_path, delimiter=\",\", dtype=str)\n",
    "  word= data[:,0]\n",
    "  proficiency = data[:,1].astype(int)/5\n",
    "  msg=[f\"{word[i]}: {proficiency[i]}, \" for i in range(len(word))]\n",
    "  return ('').join(msg)\n",
    "msg_list=load_vocab_data(\"./vocab_datas/italian.csv\")\n",
    "msg_system=\"You are a language tutor. The user is learning new words.\" \n",
    "msg_user=f\"\"\"\n",
    "Here is a list of words followed by their proficiency levels in the scale of 0 to 1.\n",
    "0 being the lowest proficiency level and 1 being the highest.\n",
    "\n",
    "{msg_list}\n",
    "\n",
    "Make italian sentences using some of the words in the list above. \n",
    "Try to prioritize words with a lower proficiency level while keeping the sentence natural.\n",
    "\n",
    "Do not say anything other than the sentence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is a list of words followed by their proficiency levels in the scale of 0 to 1.\n",
      "0 being the lowest proficiency level and 1 being the highest.\n",
      "\n",
      "di: 0.6, mezzi: 0.6, camicia: 0.6, casa: 0.6, l'aspetto: 0.6, zia: 0.6, quantitÃ : 0.6, barba: 0.6, bottiglia: 0.6, capelli: 0.6, le: 0.6, capodanno: 0.6, berretto: 0.6, maglia: 0.6, famiglia: 0.6, l'autobus: 0.6, fratello: 0.6, sorella: 0.6, il: 0.6, cugina: 0.6, cognato: 0.6, tonno: 0.6, madre: 0.6, nipote: 0.6, pesce: 0.6, guanti: 0.6, vecchio: 0.6, nero: 0.6, lunghi: 0.6, l'uovo: 0.6, nuora: 0.6, riso: 0.6, azzurro: 0.6, gli: 0.6, alto: 0.6, genitori: 0.6, fragola: 0.6, ondulati: 0.6, contenitori: 0.6, stivali: 0.6, l'aglio: 0.6, fratelli: 0.6, nonni: 0.6, marito: 0.6, divertente: 0.6, marrone: 0.6, cappotto: 0.6, lisci: 0.6, cravatta: 0.6, suoceri: 0.6, serio: 0.6, alimenti: 0.6, yogurt: 0.6, marmellata: 0.6, allegro: 0.6, lentiggini: 0.6, matrimonio: 0.6, fontana: 0.6, pollo: 0.6, figlio: 0.6, foglia: 0.6, scarpe: 0.6, zio: 0.6, rosso: 0.6, blu: 0.6, torta: 0.6, sacchetto: 0.6, giallo: 0.6, figli: 0.6, birra: 0.6, e: 0.6, completo: 0.6, colori: 0.6, treno: 0.6, metropolitana: 0.6, moglie: 0.6, maglietta: 0.6, patata: 0.6, magro: 0.6, giovane: 0.6, natura: 0.6, limone: 0.6, l'albero: 0.6, borsa: 0.6, giacca: 0.6, nuvola: 0.6, salsiccia: 0.6, l'edificio: 0.6, denti: 0.6, fetta: 0.6, bicicletta: 0.6, l'anniversario: 0.6, strada: 0.6, i: 0.6, patatine: 0.6, maglione: 0.6, prato: 0.6, d'acqua: 0.6, piedi: 0.6, gonna: 0.6, pantaloni: 0.6, l'arancia: 0.6, ginnastica: 0.6, vestito: 0.6, antipatico: 0.6, cetriolo: 0.6, occhi: 0.6, compleanno: 0.6, grigio: 0.6, nipoti: 0.6, foulard: 0.6, simpatico: 0.6, l'insalata: 0.6, nonna: 0.6, metro: 0.6, sandali: 0.6, l'aereo: 0.6, chiesa: 0.6, padre: 0.6, pacco: 0.6, sciarpa: 0.6, lo: 0.6, piccolo: 0.6, fiume: 0.6, suocera: 0.6, da: 0.6, jeans: 0.6, mucca: 0.6, calvo: 0.6, fiore: 0.6, l'auto: 0.6, ricci: 0.6, tram: 0.6, mela: 0.6, nave: 0.6, cugini: 0.6, festivitÃ : 0.6, nonno: 0.6, zii: 0.6, l'abito: 0.6, barattolo: 0.6, natale: 0.6, l'ape: 0.6, bocca: 0.6, a: 0.6, scatoletta: 0.6, noioso: 0.6, cipolla: 0.6, piccione: 0.6, arancione: 0.6, genero: 0.6, naso: 0.6, portare: 0.6, carne: 0.6, camicetta: 0.6, panchina: 0.6, pane: 0.6, trasportola: 0.6, orecchie: 0.6, corti: 0.6, pasta: 0.6, occhiali: 0.6, latte: 0.6, bianco: 0.6, figlia: 0.6, cielo: 0.6, verde: 0.6, grasso: 0.6, macchina: 0.6, mossi: 0.6, moto: 0.6, vestiti: 0.6, cugino: 0.6, basso: 0.6, suocero: 0.6, grande: 0.6, montagna: 0.6, sole: 0.6, la: 0.6, lattina: 0.6, avere: 0.6, \n",
      "\n",
      "Make italian sentences using some of the words in the list above. \n",
      "Try to prioritize words with a lower proficiency level while keeping the sentence natural.\n",
      "\n",
      "Do not say anything other than the sentence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(msg_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m   msg\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproficiency[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word))]\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(msg)\n\u001b[0;32m----> 7\u001b[0m msg_list\u001b[38;5;241m=\u001b[39m\u001b[43mload_vocab_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./vocab_datas/italian.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m msg_system\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSei un insegnante di lingue. L\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutente sta imparando nuove parole.\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m      9\u001b[0m msg_user\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mGenera una frase naturale e grammaticalmente corretta in italiano utilizzando alcune delle parole nell\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melenco sottostante.\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mCerca di dare la prioritÃ  alle parole con un livello di competenza piÃ¹ basso, mantenendo comunque la frase naturale.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mmsg_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mload_vocab_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_vocab_data\u001b[39m(file_path):\n\u001b[0;32m----> 2\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mloadtxt(file_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m      3\u001b[0m   word\u001b[38;5;241m=\u001b[39m data[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m   proficiency \u001b[38;5;241m=\u001b[39m data[:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def load_vocab_data(file_path):\n",
    "  data = np.loadtxt(file_path, delimiter=\",\", dtype=str)\n",
    "  word= data[:,0]\n",
    "  proficiency = data[:,1].astype(int)/5\n",
    "  msg=[f\"{word[i]}: {proficiency[i]}, \" for i in range(len(word))]\n",
    "  return ('').join(msg)\n",
    "msg_list=load_vocab_data(\"./vocab_data/italian.csv\")\n",
    "msg_system=\"Sei un insegnante di lingue. L'utente sta imparando nuove parole.\" \n",
    "msg_user=f\"\"\"\n",
    "Genera una frase naturale e grammaticalmente corretta in italiano utilizzando alcune delle parole nell'elenco sottostante.\n",
    "Cerca di dare la prioritÃ  alle parole con un livello di competenza piÃ¹ basso, mantenendo comunque la frase naturale.\n",
    "\n",
    "Ecco un elenco di parole seguite dal loro livello di competenza su una scala da 0 a 1,\n",
    "dove 0 rappresenta il livello di competenza piÃ¹ basso e 1 il piÃ¹ alto.\n",
    "\n",
    "{msg_list}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\", device=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_system},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_user},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_system},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_user},]\n",
    "        },\n",
    "        {'role': 'assistant',\n",
    "         'content': 'Okay, thatâ€™s fantastic! Iâ€™m ready to help you build sentences using those words.'},\n",
    "        {'role': 'user',\n",
    "         'content': 'Now generate the sentence as instructed'},\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages,min_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_system},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_user},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(messages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': [{'role': 'system',\n",
       "     'content': [{'type': 'text',\n",
       "       'text': 'You are a language tutor. The user is learning new words.'}]},\n",
       "    {'role': 'user',\n",
       "     'content': [{'type': 'text',\n",
       "       'text': \"\\nHere is a list of words followed by their proficiency levels in the scale of 0 to 1.\\n0 being the lowest proficiency level and 1 being the highest.\\n\\njeans: 0.6, l'insalata: 0.6, macchina: 0.6, lo: 0.6, fiore: 0.6, l'edificio: 0.6, blu: 0.6, ricci: 0.6, nonni: 0.6, compleanno: 0.6, pane: 0.6, gonna: 0.6, metro: 0.6, cravatta: 0.6, nero: 0.6, colori: 0.6, capodanno: 0.6, completo: 0.6, le: 0.6, limone: 0.6, nuvola: 0.6, fratello: 0.6, prato: 0.6, marito: 0.6, maglietta: 0.6, maglia: 0.6, bicicletta: 0.6, lunghi: 0.6, l'albero: 0.6, zii: 0.6, antipatico: 0.6, barattolo: 0.6, bianco: 0.6, festivitÃ : 0.6, cugino: 0.6, suoceri: 0.6, allegro: 0.6, carne: 0.6, tonno: 0.6, magro: 0.6, l'ape: 0.6, patata: 0.6, di: 0.6, rosso: 0.6, grande: 0.6, mela: 0.6, pasta: 0.6, barba: 0.6, avere: 0.6, moto: 0.6, marrone: 0.6, stivali: 0.6, gli: 0.6, l'abito: 0.6, nipoti: 0.6, moglie: 0.6, zio: 0.6, montagna: 0.6, l'aereo: 0.6, piccione: 0.6, la: 0.6, marmellata: 0.6, e: 0.6, lisci: 0.6, alto: 0.6, mezzi: 0.6, sandali: 0.6, matrimonio: 0.6, natale: 0.6, fontana: 0.6, fiume: 0.6, d'acqua: 0.6, camicia: 0.6, zia: 0.6, l'auto: 0.6, salsiccia: 0.6, birra: 0.6, nave: 0.6, ginnastica: 0.6, giacca: 0.6, scarpe: 0.6, cognato: 0.6, contenitori: 0.6, foglia: 0.6, portare: 0.6, basso: 0.6, pantaloni: 0.6, strada: 0.6, guanti: 0.6, torta: 0.6, alimenti: 0.6, denti: 0.6, lentiggini: 0.6, famiglia: 0.6, trasportola: 0.6, vecchio: 0.6, l'anniversario: 0.6, cugini: 0.6, ondulati: 0.6, sciarpa: 0.6, verde: 0.6, camicetta: 0.6, grasso: 0.6, scatoletta: 0.6, figli: 0.6, yogurt: 0.6, bocca: 0.6, l'aglio: 0.6, nuora: 0.6, fetta: 0.6, foulard: 0.6, bottiglia: 0.6, corti: 0.6, suocero: 0.6, figlio: 0.6, lattina: 0.6, vestito: 0.6, vestiti: 0.6, calvo: 0.6, azzurro: 0.6, figlia: 0.6, divertente: 0.6, madre: 0.6, giallo: 0.6, pacco: 0.6, nonno: 0.6, orecchie: 0.6, l'arancia: 0.6, cetriolo: 0.6, arancione: 0.6, l'uovo: 0.6, noioso: 0.6, berretto: 0.6, patatine: 0.6, cielo: 0.6, serio: 0.6, metropolitana: 0.6, giovane: 0.6, sacchetto: 0.6, suocera: 0.6, riso: 0.6, chiesa: 0.6, borsa: 0.6, sole: 0.6, mossi: 0.6, genitori: 0.6, nonna: 0.6, grigio: 0.6, maglione: 0.6, cugina: 0.6, occhiali: 0.6, piedi: 0.6, occhi: 0.6, capelli: 0.6, padre: 0.6, natura: 0.6, panchina: 0.6, piccolo: 0.6, fratelli: 0.6, da: 0.6, quantitÃ : 0.6, cipolla: 0.6, naso: 0.6, pollo: 0.6, il: 0.6, pesce: 0.6, genero: 0.6, latte: 0.6, l'aspetto: 0.6, casa: 0.6, simpatico: 0.6, mucca: 0.6, l'autobus: 0.6, tram: 0.6, a: 0.6, i: 0.6, fragola: 0.6, sorella: 0.6, treno: 0.6, nipote: 0.6, cappotto: 0.6, \\n\\nMake italian sentences using some of the words in the list above. \\nTry to prioritize words with a lower proficiency level while keeping the sentence natural.\\n\\nDo not say anything other than the sentence.\\n\"}]},\n",
       "    {'role': 'assistant',\n",
       "     'content': 'Il nonno ha i capelli bianchi e una barba grande.'}]}]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'I go. The cat runs. I eat. The dog is big. I read a book with a red pen. The sun is big. \\nTranslation: I go. The cat runs. I eat. The dog is big. I read a book with a red pen. The sun is big.\\nExplanation: These sentences use simple present tense and basic sentence structure (Subject-Verb-Object/Complement). \"I\" is the subject in many sentences, practicing personal pronouns. Articles (\"a\", \"the\") are used appropriately for introducing nouns. Adjectives (â€œbigâ€, â€œredâ€) modify nouns. The verb \"to be\" is used to describe characteristics (\"is big\"). Sentence structure remains simple and clear for A1 learners.\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Sentences: I go. The cat runs. I eat. The dog is big. I read a book with a red pen. The sun is big. \\nTranslation: I go. The cat runs. I eat. The dog is big. I read a book with a red pen. The sun is big.\\nExplanation: These sentences use simple present tense and basic sentence structure (Subject-Verb-Object/Complement). \\\"I\\\" is the subject in many sentences, practicing personal pronouns. Articles (\\\"a\\\", \\\"the\\\") are used appropriately for introducing nouns. Adjectives (â€œbigâ€, â€œredâ€) modify nouns. The verb \\\"to be\\\" is used to describe characteristics (\\\"is big\\\"). Sentence structure remains simple and clear for A1 learners.\\n\\n\\n\\n\"\n",
    "text.split('Sentences: ')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
